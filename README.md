# mts_shad_ML_competition-churn-prediction

https://www.kaggle.com/competitions/teta-mts-ml-1

Итоговый скор на private: 0.65263 (10 место)

Предобработка:
  - Для удобства переименовал фитчи.
  - Дополнительно сгенерил дополнительные фитчи: соотношения и мультипликация некоторых величин. Соотношения: сумма_пополнения_баланса\выручка_за_квартал к доходу, доход\выручка_за_квартал к частоте, сумма_пополнения_баланса к частоте_пополнения_баланса. Мультипликация: сумма_пополнения_баланса на частоту_пополнения_баланса, доход\выручка_за_квартал на частоту. (Идея соотношений: просто посмотреть на отношения данных величин, идея мультипликаций: вывести новые суммарные значения\показатели по прибыли от клиента.
  - Заполнение NaN. Считаю, что отсутствие фитчей - тоже важный показатель, так что данные с NaN оставил: пропуски в численных фитчах заполнил значениями -1000, в категориальных - "NO_DATA".
  - Произвел скейлинг численных фитчей с помощью StandardScaler() и кодирование категориальных с помощью OneHotEncoder(). После кодирования категориальных фитчей пришлось обработать названия колонок в таблице (избавиться от лишних символов в названиях). 

Обучение моделей:
  Попробовал различные бустинги: LightGBM, CatBoost, XGBoost (без подбора параметров они выдавали скор ~0.6). Сначала для каждого пробовал руками подбирать параметры, вдруг прокатит - прокатило: с первой попытки в public побил baseline_boosting. После уже с помощью optuna тюнил параметры. Единственный параметр, который я подобрал руками и не менял во всех моделях (точнее, пробовал менять, но было значительно хуже), - это "scale_pos_weight": 1.9. Остальные уже искал optunа'ой, задавая поиск параметров в пределах, которые мне казались наилучшими (основываясь на том, что смог руками подобрать).
  Далее ансамблирование моделей: пробовал Voting и Stacking, результаты были примерно одинаковые, решил оставить VotingClassifier.
  Для улучшения скора возможно попробовать добавить к финальному ансамблю lightautoml и knn, либо алгоритмы схожие с ним.
